{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch # 导入的是 torch 而不是 pytorch\n",
    "print(torch.__version__) # 输出当前pytorch的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "# 读入数据\n",
    "file_path = 'fer2013.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集划分为训练和测试模块\n",
    "X_train, Y_train, X_test, Y_test = [], [], [], []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    val = row['pixels'].split(\" \")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "            X_train.append(np.array(val, 'float32'))\n",
    "            Y_train.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "            X_test.append(np.array(val, 'float32'))\n",
    "            Y_test.append(row['emotion'])\n",
    "    except:\n",
    "        print(f\"error occured at index :{index} and row:{row}\")\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "X_train = np.array(X_train, 'float32')\n",
    "Y_train = np.array(Y_train, 'float32')\n",
    "X_test = np.array(X_test, 'float32')\n",
    "Y_test = np.array(Y_test, 'float32')\n",
    "\n",
    "# 将0-6的标签数据转化为onehot数据\n",
    "def to_onehot(y):\n",
    "    temp_outs = np.zeros((y.shape[0], np.unique(y).size), dtype=np.uint8)\n",
    "    temp_outs[np.arange(y.shape[0]), np.uint8(y)] = 1\n",
    "    return temp_outs\n",
    "Y_train = to_onehot(Y_train)\n",
    "Y_test = to_onehot(Y_test)\n",
    "# print(Y_train)\n",
    "# print(Y_test)\n",
    "\n",
    "# normalizing data between 0 and 1\n",
    "X_train -= np.mean(X_train, axis=0)\n",
    "X_train /= np.std(X_train, axis=0)\n",
    "\n",
    "X_test -= np.mean(X_test, axis=0)\n",
    "X_test /= np.std(X_test, axis=0)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(\n",
    "        self, features: nn.Module, num_classes: int = 1000, init_weights: bool = True, dropout: float = 0.5\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        torch._C._log_api_usage_once(self)\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight, 0, 0.01)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def make_layers(cfg: List[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "model = VGG(make_layers(cfg, batch_norm=True))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGNet(\n",
      "  (conv1_1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=18432, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fc3): Linear(in_features=1024, out_features=7, bias=True)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as  plt\n",
    "import torch.nn  as  nn\n",
    "import torch.utils.data as Data \n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from functools import partial\n",
    "from typing import Union, List, Dict, Any, Optional, cast\n",
    "\n",
    "device=torch.device('cuda:0')\n",
    "\n",
    "# X_train = torch.tensor(X_train).to(device)\n",
    "# X_test = torch.tensor(X_test).to(device)\n",
    "# Y_train = torch.tensor(Y_train).to(device)\n",
    "# Y_test = torch.tensor(Y_test).to(device)\n",
    "\n",
    "train_set = Data.TensorDataset(X_train, Y_train)\n",
    "train_loader = Data.DataLoader(dataset=train_set, shuffle=True, batch_size=batch_size, num_workers=2)\n",
    "test_set = Data.TensorDataset(Y_test, Y_test)\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(6*6*512, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 7)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(1024)\n",
    "\n",
    "    # 定义前向传播过程，输入为x\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(self.conv1_1(x)))\n",
    "        x = self.bn1(F.relu(self.conv1_2(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.bn2(F.relu(self.conv2_1(x)))\n",
    "        x = self.bn2(F.relu(self.conv2_2(x)))\n",
    "        x = F.dropout2d(self.pool(x), 0.2)\n",
    "\n",
    "        x = self.bn3(F.relu(self.conv3_1(x)))\n",
    "        x = self.bn3(F.relu(self.conv3_2(x)))\n",
    "        x = F.dropout2d(self.pool(x), 0.25)\n",
    "\n",
    "        x = self.bn4(F.relu(self.conv4_1(x)))\n",
    "        x = self.bn4(F.relu(self.conv4_2(x)))\n",
    "        x = F.dropout2d(self.pool(x), 0.25)\n",
    "\n",
    "        x = x.reshape(-1, 6*6*512)\n",
    "        # x = x.flatten()\n",
    "        print(x.shape)\n",
    "        x = self.fc1(x).flatten()\n",
    "        print(x.shape)\n",
    "        x = F.dropout(F.relu(self.bn5(x)), 0.45)\n",
    "        x = F.dropout(F.relu(self.bn5(self.fc2(x))), 0.45)\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "net = VGGNet().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 18432])\n",
      "torch.Size([16384])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 2D or 3D input (got 1D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - mail2.sysu.edu.cn\\1SYSU\\Programming\\AI\\大作业\\AI_LessonProject\\train_pytorch.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20mail2.sysu.edu.cn/1SYSU/Programming/AI/%E5%A4%A7%E4%BD%9C%E4%B8%9A/AI_LessonProject/train_pytorch.ipynb#ch0000006?line=6'>7</a>\u001b[0m batch_x \u001b[39m=\u001b[39m batch_x\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m48\u001b[39m,\u001b[39m48\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20mail2.sysu.edu.cn/1SYSU/Programming/AI/%E5%A4%A7%E4%BD%9C%E4%B8%9A/AI_LessonProject/train_pytorch.ipynb#ch0000006?line=7'>8</a>\u001b[0m batch_y \u001b[39m=\u001b[39m batch_y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20mail2.sysu.edu.cn/1SYSU/Programming/AI/%E5%A4%A7%E4%BD%9C%E4%B8%9A/AI_LessonProject/train_pytorch.ipynb#ch0000006?line=8'>9</a>\u001b[0m output \u001b[39m=\u001b[39m net(batch_x)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20mail2.sysu.edu.cn/1SYSU/Programming/AI/%E5%A4%A7%E4%BD%9C%E4%B8%9A/AI_LessonProject/train_pytorch.ipynb#ch0000006?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, batch_y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20mail2.sysu.edu.cn/1SYSU/Programming/AI/%E5%A4%A7%E4%BD%9C%E4%B8%9A/AI_LessonProject/train_pytorch.ipynb#ch0000006?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\OneDrive - mail2.sysu.edu.cn\\1SYSU\\Programming\\AI\\大作业\\AI_LessonProject\\train_pytorch.ipynb Cell 5'\u001b[0m in \u001b[0;36mVGGNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20mail2.sysu.edu.cn/1SYSU/Programming/AI/%E5%A4%A7%E4%BD%9C%E4%B8%9A/AI_LessonProject/train_pytorch.ipynb#ch0000004?line=69'>70</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20mail2.sysu.edu.cn/1SYSU/Programming/AI/%E5%A4%A7%E4%BD%9C%E4%B8%9A/AI_LessonProject/train_pytorch.ipynb#ch0000004?line=70'>71</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20mail2.sysu.edu.cn/1SYSU/Programming/AI/%E5%A4%A7%E4%BD%9C%E4%B8%9A/AI_LessonProject/train_pytorch.ipynb#ch0000004?line=71'>72</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn5(x)), \u001b[39m0.45\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20mail2.sysu.edu.cn/1SYSU/Programming/AI/%E5%A4%A7%E4%BD%9C%E4%B8%9A/AI_LessonProject/train_pytorch.ipynb#ch0000004?line=72'>73</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn5(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))), \u001b[39m0.45\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20mail2.sysu.edu.cn/1SYSU/Programming/AI/%E5%A4%A7%E4%BD%9C%E4%B8%9A/AI_LessonProject/train_pytorch.ipynb#ch0000004?line=73'>74</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\batchnorm.py:135\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    137\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\batchnorm.py:298\u001b[0m, in \u001b[0;36mBatchNorm1d._check_input_dim\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 298\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexpected 2D or 3D input (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim())\n\u001b[0;32m    300\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: expected 2D or 3D input (got 1D input)"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        batch_x = batch_x.reshape(-1,1,48,48).to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        output = net(batch_x).to(device)\n",
    "        loss = criterion(output, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7484d2d05136da1b864a797d299b7c4b86bf21fd2bd86a404ee2e7691594036"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
